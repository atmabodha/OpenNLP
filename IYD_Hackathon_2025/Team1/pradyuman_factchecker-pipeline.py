import re
import json
import time
from datetime import timedelta

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

import numpy as np
import pandas as pd
import unicodedata
from tqdm import tqdm
from collections import defaultdict

import faiss
import nltk
from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize
import string
from sentence_transformers import SentenceTransformer, CrossEncoder
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import CrossEncoder

# to use Mistral generate a api token by yourself or if this works then fine (its generated by me)
from huggingface_hub import login


login(token="")

# Check if GPU is availabel
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Device selected: {device}')

############################################################################################################
#                                            Verse Processing
############################################################################################################
# Specify the Scraped data CSV file
DATA_PATH = 'valmiki-ramayana-verses.csv'
df = pd.read_csv(DATA_PATH)

# renaming the columns for easy handling
df = df.rename(columns={ 
            'Kanda/Book': 'Book',
            'Sarga/Chapter': 'Chapter',
            'Shloka/Verse Number': 'Verse_number', 
            'English Translation': 'Verse_Text',
            })
df['ID'] = df['Book'].astype(str) + '-' + df['Chapter'].astype(str) + '-' + df['Verse_number'].astype(str)
df = df[['ID'] + [col for col in df.columns if col != 'ID']]

# handles the different aliases of name entities
VARIANT_MAP = {
    "seetha": "sita", "seeta": "sita", "maithili": "sita", "janaki": "sita", "vaidehi": "sita",
    "ram": "rama", "raghava": "rama",
    "hanuma": "hanuman", 
    "lakshman": "lakshmana"
}

# normalize the text for model input
def normalize(text):
    # Unicode normalization
    text = unicodedata.normalize("NFKC", text).encode('ASCII', 'ignore').decode('utf-8')
    
    # Lowercase
    text = text.lower()

    # Remove quotes not part of contractions or possessives
    text = re.sub(r"(?<=\s)'|'(?=\s)|^'|'$", "", text)

    # Remove unwanted punctuation, keeping . , / - ' : ; ? ! " ( ) and apostrophes in contractions/in-text
    text = re.sub(r"[^\w\s'.,/\-:;?!\()]", "", text)

    # Normalize ellipses
    text = re.sub(r"\.{2,}", ".", text)

    # Remove punctuation at start/end if isolated
    text = re.sub(r"^[.,\s]+|[.,\s]+$", "", text)

    # 4. Apply variant mapping
    for variant, canonical in VARIANT_MAP.items():
        # Match variant in word-boundary-aware regex, allow possessive `'s`
        pattern = re.compile(fr"\b{variant}\b(?=(?:[']s)?[.,;:?!\"]*|\s|$)", flags=re.IGNORECASE)
        text = pattern.sub(canonical, text)

    return text

# Create a list of chunk dicts
chunks = []
for _, row in df.iterrows():
    try:
        text = row["Verse_Text"]
        if not pd.isna(text):
            chunks.append({
                "id": row["ID"],
                "text": normalize(text),
                "meta": {
                    "book": row["Book"],
                    "chapter": row["Chapter"],
                    "verse_num": row["Verse_number"]
                }
            })
    except Exception as e:
        print(row)
        raise Exception(str(e))

############################################################################################################
#                                           Verse Encoding & Indexing
############################################################################################################

# Load the embedding model
print('\nLoading embedding Model...')
embedder = SentenceTransformer("intfloat/e5-large-v2", device=device)
print('\nEmbedder Model Loaded.')

# Prepare inputs
texts = [chunk['text'] for chunk in chunks]

# Encode and normalize
print('\nGenerating embeddings...')
embeddings = embedder.encode(texts, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=True)

# Save to disk
np.save("ramayana_embeddings.npy", embeddings)
print(f'\nEmbeddings generated and saved successfully')

########################################
# Build FAISS Index (Dense Retrival)
########################################

print('\nBuilding FAISS index...')
# Build FAISS index (cosine similarity via inner product)
dim = embeddings.shape[1]
faiss_index = faiss.IndexFlatIP(dim)
faiss_index.add(embeddings)
print("FAISS index ready.")

######################################
# Build BM25 Index (Sparse Retrival)
######################################

nltk.download("punkt")
nltk.download('punkt_tab')

print('\nBuilding BM25 index...')
# Remove punctuation and tokenize
tokenized_corpus = [
    [token for token in word_tokenize(chunk["text"]) if token not in string.punctuation]
    for chunk in chunks
]

# Build BM25 index
bm25 = BM25Okapi(tokenized_corpus)
print("BM25 index ready.")

############################################################################################################
#                            Hybrid Retrival Engine - RAG (Retrieval-Augmented Generation) 
############################################################################################################
print('########################################')
print('  Building Hybrid RAG Retrival Engine')
print('########################################')

#####################################################################
# Ensemble Retriever: FAISS + BM25 -> Reciprocal Rank Fusion (RRF)
#####################################################################

def get_faiss_ranks(query, top_k=10, sem_threshold=0.8): # if faiss list is empty then return None even if bm25 return something
    query_emb = embedder.encode(query, normalize_embeddings=True, show_progress_bar=True)
    D, I = faiss_index.search(np.array([query_emb]), top_k)
    return [int(idx) for idx, score in zip(I[0], D[0]) if score > sem_threshold]

def get_bm25_ranks(query, top_k=10):
    query_tokens = word_tokenize(query)
    scores = bm25.get_scores(query_tokens)
        
    # Filter for scores > 0 only
    scored_results = [(i, s) for i, s in enumerate(scores) if s > 0]
    if not scored_results:
        return []  # No meaningful BM25 results
    
    # Sort by score
    ranked = sorted(enumerate(scored_results), key=lambda x: x[1][1], reverse=True)[:top_k]
    return [int(_[0]) for i, _ in ranked]

class EnsembleRetriever:
    def __init__(self, retrievers: list, k_rrf: int = 60):
        """
        Args:
            retrievers: list of retriever objects with .retrieve(query, top_k) â†’ list[int]
            k: RRF constant (default = 60)
        """
        self.retrievers = retrievers
        self.k_rrf = k_rrf

    def _rrf(self, rank):
        return 1 / (self.k_rrf + rank)

    def retrieve(self, top_n=10):
        rrf_scores = defaultdict(float)
        for rank_list in self.retrievers:
            for rank, doc_id in enumerate(rank_list):
                rrf_scores[doc_id] += self._rrf(rank+1)
        ranked = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]

        results = []
        for idx, score in ranked:
            results.append({
                "idx": idx,
                "id": chunks[idx]["id"],
                "book": chunks[idx]["meta"]["book"],
                "chapter": chunks[idx]["meta"]["chapter"],
                "text": chunks[idx]["text"],
                "score": score
            })
        return results        

################################################
# Cross Encoder Reranking
################################################  

print('Load CrossEncoder Model')
# Load the cross-encoder
rank_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')

def get_cross_encoder_ranks(query, retrived_verses, top_n=10):
    # Create pairs of (query, verse)
    pairs = [(query, verse['text']) for verse in retrived_verses] 

    # Get relevance scores
    scores = rank_model.predict(pairs)

    # Combine verses with their scores and sort them
    ranked = sorted(zip(retrived_verses, scores), key=lambda x: x[1], reverse=True)

    return [item[0] for item in ranked][:top_n]

############################################################################################################
#                                   Claim Processing and Prediction 
############################################################################################################

##############################################
# Initialize Mistral-7B-Instruct-v0.2 model
##############################################

class ClaimVerifier:
    def __init__(self, model_name="mistralai/Mistral-7B-Instruct-v0.2", device=None):
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Loading model on {self.device}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            device_map="auto", 
            torch_dtype=torch.float16
        )
        self.model.eval()

    def build_prompt(self, claim, verses):
        verses_text = "\n".join([f"{i}. {v['text']}" for i, v in enumerate(verses, 1)])
        prompt = f"""
            <s>[INST]
            You are a scholarly expert in the Valmiki Ramayana, the ancient Sanskrit epic. Your task is to fact-check the given **claim** about Ramayana content using the **textual evidence from provided verses**.
                       
            You must output a JSON object in the following **STRICT format**:
            {{
              "relevance": "RAMAYANA_RELATED" or "NOT_RAMAYANA_RELATED",
              "label": "TRUE" or "FALSE",
              "confidence_score": 1-10,
              "reference": [list of verse indices most relevant to your decision],
              "explanation": "A very brief explanation (1â€“2 sentences) of why the claim is TRUE or FALSE based on the provided textual evidence with verses number."
            }}

            **Guidelines:**
                 
            **Relevance Assessment:**
            - Set `relevance` to "RAMAYANA_RELATED" if the claim refers to characters (e.g., Rama, Sita, Hanuman, Ravan, etc), places (e.g., Ayodhya, Lanka, etc), events (e.g., exile, war, abduction, etc), or concepts (e.g., dharma, vanavas, etc) that occur in the Valmiki Ramayana â€” even if the claim is incorrect or imprecise.
            - Set `relevance` to "NOT_RAMAYANA_RELATED" if the claim is clearly unrelated â€” e.g., modern facts ("The capital of India is Delhi"), people not in Ramayana ("Ronaldo plays football"), or topics not covered in the Valmiki text.
    
            **CONFIDENCE SCORING (1-10) - IMPORTANT:**
            The confidence score represents how certain you are about your TRUE or FALSE decision:
            - **9-10**: Clear, direct evidence supports OR contradicts the claim. You are 90-100% likely your TRUE or FALSE answer is correct based on the evidence.
            - **7-8**: Strong evidence supports OR contradicts the claim. You are 80-89% likely your TRUE or FALSE answer is correct based on the evidence.
            - **5-6**: Some relevant but indirect evidence. You are 60-79% likely your TRUE or FALSE answer is correct based on the evidence.
            - **3-4**: Weak/ambiguous evidence. You are 30-59% likely your TRUE or FALSE answer is correct based on the evidence.
            - **0-2**: No relevant evidence found.You are 0-29% likely your TRUE or FALSE answer is correct based on the evidence.
            
            **CLAIM:**
            {claim}
            
            **TEXTUAL EVIDENCE:**
            {verses_text}
                        
            Now, based on your reasoning, output only the JSON object.
            [/INST] {{
        """
        return prompt

    def verify_claim(self, claim, verses):
        prompt = self.build_prompt(claim, verses)
        #torch.cuda.empty_cache()
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(self.model.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            temperature=0,
            pad_token_id=self.tokenizer.eos_token_id,
            eos_token_id=self.tokenizer.eos_token_id
        )
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        return response

############################################################################################################
#                                       Output Handling 
############################################################################################################

###########################
# JSON Parser
###########################

# JSON Parser to extract the json object from model raw response
def extract_fact_check_json(text: str):
    # Decode escape characters if any
    text = text.encode().decode("unicode_escape")
    
    # Split on the closing instruction tag
    parts = text.split('[/INST]')
    if len(parts) < 2:
        raise ValueError("No closing [/INST] tag found.")
    
    # Take everything after [/INST]
    json_part = parts[1].strip()
    
    # Extract the first JSON object in this part
    match = re.search(r'\{[\s\S]*?\}', json_part)
    if not match:
        raise ValueError("No JSON object found after [/INST].")
    
    json_str = match.group(0)
    
    # Parse JSON
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        raise ValueError(f"Failed to parse JSON: {e}\nJSON string:\n{json_str}")

############################################
# Postprocess and Cleaned parsed Output ðŸ§¹
############################################

# Function to map the supporting verse back to the reference in a described format Book, Sarga _, Shloka _ e.g. Bala Kanda, Sarga 5, Shloka 9
def extract_ref(json, top_verses, chunks):
    if not json.get('reference'):
        return json  # If the list is empty or key doesn't exist, return as-is

    updated_refs = []
    for s in json['reference']:
        if 0 <= s < len(top_verses):  # Ensure index is valid
            idx = top_verses[s-1]['idx']
            meta = chunks[idx]['meta']
            formatted_ref = f"{meta['book']}, Sarga {meta['chapter']}, Shloka {meta['verse_num']}"
            updated_refs.append((s, formatted_ref))

    json['reference'] = updated_refs
    return json

# Regex to match references like "verse 1", "verses 2 and 3", "verse 4-5", etc.
reference_pattern = re.compile(r"""
    (?:
        [\(\[\{]?\s*                         # optional opening ( [ {
        (?:(?i:verses?|references?)[:\s]*)? # optional prefix
        (
            \d+(?:\s*-\s*\d+)?              # number or range
            (?:\s*,\s*\d+(?:\s*-\s*\d+)?)*  # , more numbers or ranges
            (?:\s*(?:,?\s*and\s*)\d+(?:\s*-\s*\d+)?)?  # optional and N
        )
        \s*[\)\]\}]?                         # optional closing ) ] }
    )
""", flags=re.VERBOSE | re.IGNORECASE)

# Helper to extract list of verse numbers
def expand_numbers(num_str):
    nums = []
    for part in re.split(r",|\band\b", num_str):
        part = part.strip()
        if "-" in part:
            start, end = map(int, part.split("-"))
            nums.extend(range(start, end + 1))
        elif part.isdigit():
            nums.append(int(part))
    return nums

# Replace matched reference with metadata
def replacement_func(match):
    num_str = match.group(1)
    verse_nums = expand_numbers(num_str)
    replacements = [dict(predicted_ref).get(num, f"[Verse {num} missing]") for num in verse_nums]
    return "(" + "; ".join(replacements) + ")"

# processing the explanation for replacing the verse no. with proper reference
def explanation_processing(predicted_json):
    global predicted_ref
    predicted_exp = predicted_json['explanation']
    if 'reference' in predicted_json:
        predicted_ref = predicted_json['reference']
    else:
        predicted_ref = [] 
    
    # Replace all references in text
    final_exp = re.sub(reference_pattern, replacement_func, predicted_exp)
    return final_exp

############################################################################################################
#                                       FactChecker Pipeline in Action ðŸ˜Ž 
############################################################################################################

if __name__ == '__main__':
    # Load test data
    df_test = pd.read_csv('RamayanaFinal.csv') # change file name as necessary
    results = []
    
    # Mistral Model call
    verifier = ClaimVerifier()

    total_time = 0
    n = 0
    print('\nRunning Pipeline...\n')
    for idx, row in tqdm(df_test.iterrows(), desc='Predicting...'):
        claim = row['Statement']
        
        start = time.perf_counter()
        
        query = normalize(claim)
        faiss_ranks = get_faiss_ranks(query, 100, sem_threshold=0.80)
        bm25_ranks = get_bm25_ranks(query, 100)

        # If the similarity score is below the 0.8 threshold the verse is irrelevant
        if faiss_ranks:
            # Hybrid RAG Engine
            retriever = EnsembleRetriever([faiss_ranks, bm25_ranks])
            retrived_verses = retriever.retrieve(top_n=100)

            # Reranking retrived verses (Cross Encoder)
            top_verses = get_cross_encoder_ranks(query, retrived_verses, top_n=10)
            
            # Generate raw output from the LLM
            raw_response = verifier.verify_claim(query, top_verses)
            end = time.perf_counter()

            total_time += (end - start)
            n += 1

            # Parse JSON result
            result_json = extract_fact_check_json(raw_response)
            predicted_json = extract_ref(result_json, top_verses, chunks)

            print(f"\n=== User Claim: {claim} ===\n")
            print("=== Model Output ===")
            print("RELEVANCE: ", predicted_json['relevance'])

            # Save final result with reference ONLY if predicted True
            if predicted_json['relevance'] == 'NOT_RAMAYANA_RELATED':
                predicted_truth = 'NOT RELEVANT'
                predicted_ref = np.nan
                model_xplanation = 'Not relevant to the Ramayana verses'
            else:
                # processing the explanation for replacing the verse no. with proper reference
                final_explanation = explanation_processing(predicted_json)
                if str(predicted_json["label"]).strip().upper() == "TRUE":
                    predicted_truth = predicted_json['label']
                    predicted_ref = predicted_json["reference"]
                    model_xplanation = final_explanation
                elif str(predicted_json["label"]).strip().upper() == "FALSE":
                    predicted_truth = predicted_json['label']
                    predicted_ref = np.nan
                    model_xplanation = final_explanation
                else:
                    predicted_ref = np.nan

            results.append({
                #"ID": row["ID"],
                "Statement": claim,
                #"Truth": row['Truth'], # Actual verdict
                "Prediction": predicted_truth, # Predicted vertict
                #"Reference": row['Reference'], # Actual Reference
                #"Predicted_Reference": predicted_ref, # Predicted reference
                "Explanation": model_xplanation # Model Explanation
            })

            print(f"PREDICTION: {predicted_truth}")
            print(f"Explanation: {model_xplanation}")
            print("====================")
        else:
            results.append({
                #"ID": row["ID"],
                "Statement": claim,
                #"Truth": row['Truth'], # Actual verdict
                "Prediction": 'NOT RELEVANT', # Explicit verdict
                #"Reference": np.nan,
                #"Predicted_Reference": np.nan,
                "Explanation": 'Not relevant to the Ramayana verses'
            })

    print('\nPredictions Complete...')

    # Saving the output to csv
    results_df = pd.DataFrame(results)
    results_df.to_csv("output.csv", index=False)
    print(f'Predictions saved as output.csv file.')

    # Average inference time per Statement
    average_time = total_time / n if n > 0 else 0

    total_time_formatted = str(timedelta(seconds=total_time))
    average_time_formatted = str(timedelta(seconds=average_time))

    print(f"\nTotal verses: {n}")
    print(f"Total execution time: {total_time_formatted}")
    print(f"Average execution time: {average_time_formatted}")


##########################################################################################################
#                                             THANK YOU ðŸ™ƒ
##########################################################################################################

# Code by Pradyuman Thakur











